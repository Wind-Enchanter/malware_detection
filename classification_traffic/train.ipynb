{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.rcParams['font.size'] = 18\n",
    "matplotlib.rcParams['figure.titlesize'] = 18\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 7]\n",
    "matplotlib.rcParams['font.family'] = ['KaiTi']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "import  os\n",
    "import  tensorflow as tf\n",
    "import  numpy as np\n",
    "from tensorflow.keras import layers, optimizers, Sequential, metrics\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "assert tf.__version__.startswith('2.')\n",
    "\n",
    "import sys\n",
    "sys.path.append('D:/USTC-TK2016/')\n",
    "from load_dataset import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x,y):\n",
    "    # x: 图片的路径，y：图片的数字编码\n",
    "    x = tf.io.read_file(x)\n",
    "    x = tf.image.decode_jpeg(x, channels=1) # RGBA\n",
    "    x = tf.image.resize(x, [28, 28])\n",
    "\n",
    "#     x = tf.image.random_flip_left_right(x)\n",
    "#     x = tf.image.random_flip_up_down(x)\n",
    "#     x = tf.image.random_crop(x, [224,224,3])\n",
    "\n",
    "    # x: [0,255]=> -1~1\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.convert_to_tensor(y)\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 1024\n",
    "# 创建训练集Datset对象\n",
    "images, labels, table = load_data('D:/USTC-TK2016/4_Png/testtest',mode='train')\n",
    "db_train = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "db_train = db_train.map(preprocess).batch(batchsz).repeat(10)\n",
    "# 创建验证集Datset对象\n",
    "images2, labels2, table = load_data('D:/USTC-TK2016/4_Png/testtest',mode='val')\n",
    "db_val = tf.data.Dataset.from_tensor_slices((images2, labels2))\n",
    "db_val = db_val.map(preprocess).batch(batchsz).repeat(10)\n",
    "# 创建测试集Datset对象\n",
    "images3, labels3, table = load_data('D:/USTC-TK2016/4_Png/testtest',mode='test')\n",
    "db_test = tf.data.Dataset.from_tensor_slices((images3, labels3))\n",
    "db_test = db_test.map(preprocess).batch(batchsz).repeat(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<RepeatDataset shapes: ((None, 28, 28, 1), (None, 10)), types: (tf.float32, tf.float32)>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  3212288   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  10250     \n",
      "=================================================================\n",
      "Total params: 3,274,634\n",
      "Trainable params: 3,274,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "cnn_layers = [\n",
    "    layers.Conv2D(32, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "    layers.Conv2D(64, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1024, activation=tf.nn.relu),\n",
    "    layers.Dense(10, activation=tf.nn.softmax)\n",
    "]\n",
    "\n",
    "model = Sequential(cnn_layers)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.CategoricalCrossentropy(), \n",
    "             metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "model.build(input_shape=(None, 28, 28, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd5 in position 174: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-d9b48b5de37c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mhistory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdb_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msteps_per_epoch\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdb_val\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_steps\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m25\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001B[0m\n\u001B[0;32m    726\u001B[0m         \u001B[0mmax_queue_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmax_queue_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    727\u001B[0m         \u001B[0mworkers\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mworkers\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 728\u001B[1;33m         use_multiprocessing=use_multiprocessing)\n\u001B[0m\u001B[0;32m    729\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    730\u001B[0m   def evaluate(self,\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001B[0m\n\u001B[0;32m    222\u001B[0m           \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalidation_data\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m           \u001B[0mvalidation_steps\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalidation_steps\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 224\u001B[1;33m           distribution_strategy=strategy)\n\u001B[0m\u001B[0;32m    225\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    226\u001B[0m       \u001B[0mtotal_samples\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_get_total_number_of_samples\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtraining_data_adapter\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001B[0m in \u001B[0;36m_process_training_inputs\u001B[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m    545\u001B[0m         \u001B[0mmax_queue_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmax_queue_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    546\u001B[0m         \u001B[0mworkers\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mworkers\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 547\u001B[1;33m         use_multiprocessing=use_multiprocessing)\n\u001B[0m\u001B[0;32m    548\u001B[0m     \u001B[0mval_adapter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    549\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mvalidation_data\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001B[0m in \u001B[0;36m_process_inputs\u001B[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m    592\u001B[0m         \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    593\u001B[0m         \u001B[0mcheck_steps\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 594\u001B[1;33m         steps=steps)\n\u001B[0m\u001B[0;32m    595\u001B[0m   adapter = adapter_cls(\n\u001B[0;32m    596\u001B[0m       \u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36m_standardize_user_data\u001B[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001B[0m\n\u001B[0;32m   2417\u001B[0m     \u001B[1;31m# First, we build the model on the fly if necessary.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2418\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2419\u001B[1;33m       \u001B[0mall_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_input\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdict_inputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_build_model_with_inputs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2420\u001B[0m       \u001B[0mis_build_called\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2421\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36m_build_model_with_inputs\u001B[1;34m(self, inputs, targets)\u001B[0m\n\u001B[0;32m   2576\u001B[0m     \u001B[1;31m# tensors from the iterator and then standardize them.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2577\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mdataset_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDatasetV1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdataset_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDatasetV2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2578\u001B[1;33m       \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtargets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtraining_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract_tensors_from_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2579\u001B[0m     \u001B[1;31m# We type-check that `inputs` and `targets` are either single arrays\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2580\u001B[0m     \u001B[1;31m# or lists of arrays, and extract a flat list of inputs from the passed\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001B[0m in \u001B[0;36mextract_tensors_from_dataset\u001B[1;34m(dataset)\u001B[0m\n\u001B[0;32m   1580\u001B[0m   \"\"\"\n\u001B[0;32m   1581\u001B[0m   \u001B[0miterator\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1582\u001B[1;33m   \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtargets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample_weight\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0munpack_iterator_input\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1583\u001B[0m   \u001B[1;32mreturn\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtargets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1584\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001B[0m in \u001B[0;36munpack_iterator_input\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m   1594\u001B[0m   \"\"\"\n\u001B[0;32m   1595\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1596\u001B[1;33m     \u001B[0mnext_element\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_next\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1597\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOutOfRangeError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1598\u001B[0m     raise RuntimeError('Your dataset iterator ran out of data; '\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001B[0m in \u001B[0;36mget_next\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m    735\u001B[0m     \"\"\"\n\u001B[0;32m    736\u001B[0m     \u001B[1;32mdel\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 737\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_internal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    738\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    739\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_gather_saveables_for_checkpoint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001B[0m in \u001B[0;36m_next_internal\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    649\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_iterator_resource\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    650\u001B[0m             \u001B[0moutput_types\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_flat_output_types\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 651\u001B[1;33m             output_shapes=self._flat_output_shapes)\n\u001B[0m\u001B[0;32m    652\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    653\u001B[0m       \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\u001B[0m in \u001B[0;36miterator_get_next_sync\u001B[1;34m(iterator, output_types, output_shapes, name)\u001B[0m\n\u001B[0;32m   2656\u001B[0m         \u001B[0m_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_context_handle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_thread_local_data\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice_name\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2657\u001B[0m         \u001B[1;34m\"IteratorGetNextSync\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_post_execution_callbacks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2658\u001B[1;33m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001B[0m\u001B[0;32m   2659\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2660\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xd5 in position 174: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "history = model.fit(db_train, epochs=4, steps_per_epoch=5, validation_data=db_val, validation_steps=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save('traffic_class.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# y_pred = model.predict_classes(db_test)\n",
    "# print(\"CNN_LSTM model classification report: \\n\\n {}\".format(classification_report(np.array(db_val), y_pred.flatten())))\n",
    "#\n",
    "# fig, ax = plt.subplots(1,2, figsize=[12,6])\n",
    "# ax[0].plot(history.history[\"loss\"])\n",
    "# ax[0].plot(history.history[\"val_loss\"])\n",
    "# ax[0].set_title(\" Loss\")\n",
    "# ax[0].legend((\"Training\", \"validation\"), loc=\"upper right\")\n",
    "# ax[0].set_xlabel(\"Epochs\")\n",
    "# ax[1].plot(history.history[\"categorical_accuracy\"])\n",
    "# ax[1].plot(history.history[\"val_categorical_accuracy\"])\n",
    "# ax[1].legend((\"Training\", \"validation\"), loc=\"lower right\")\n",
    "# ax[1].set_title(\"Accuracy\")\n",
    "# ax[1].set_xlabel(\"Epochs\")\n",
    "#\n",
    "# ax=skplt.metrics.plot_confusion_matrix(db_val, y_pred, figsize=(8, 7))\n",
    "# tickx=ax.set_xticklabels(['Benign', 'Malware'])\n",
    "# ticky=ax.set_yticklabels(['Benign', 'Malware'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.evaluate(db_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}